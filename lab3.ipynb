{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Uczenie głębokie z wykorzystaniem dedykowanych narzędzi\n",
    "\n",
    "### Spis treści:\n",
    "    1) Wstęp: oprogramowanie dedykowane sieciom neuronowym\n",
    "    2) Wstep do obliczeń symbolicznych\n",
    "        2.0) teoria\n",
    "        2.1) ćwiczenie\n",
    "    3) Keras\n",
    "        3.0) prezentacja framework'u\n",
    "        3.1) ćwiczenie: implementacja modelu\n",
    "        3.2) implementacja warstw\n",
    "        3.3) ćwiczenie: implementacja własnej warstwy\n",
    "    4.) Hands-on computer vision: prezentacja i ćwiczenie z rozponawania obrazu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.)  Wstęp: oprogramowanie dedykowane sieciom neuronowym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wraz z rosnącym zainteresowaniem sieciami neuronowymi, rosło zapotrzebowanie na dedykowane narzędzia, które ułatwiłyby i przyspieszyły proces tworzenia, a następnie uczenia głębokich modeli. \n",
    "\n",
    "\n",
    "- Z upływem lat kolejne grupy badawcze prezentowały swoje rozwiązania, a w ostatnim czasie tematem zainteresował się również przemysł. Dzięki nowym źródłom finansowania i niesłabnącemu zapotrzebowaniu, większość frameworków jest obecnie aktywnie rozwijana i ulepszana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do najpopularniejszych narzędzi należą:\n",
    "\n",
    "    - [Tensorflow](http://tensorflow.org) (**Google**)\n",
    "        - Python, Go, C++\n",
    "        - Najpopularniejszy z obecnie istniejących frameworków\n",
    "        - Najintensywniej rozwijany\n",
    "\n",
    "    - [Theano](http://deeplearning.net/software/theano/) (**U. Montreal**)\n",
    "        - Python\n",
    "        - Był prekursorem TensorFlow\n",
    "        - Rozwijany przez uniwersytet w Montrealu (non-profit)\n",
    "        - Obecnie coraz częściej porzucany na rzecz TF\n",
    "\n",
    "    - [Torch](http://torch.ch/) (**Facebook, Twitter**)\n",
    "        - Lua\n",
    "        - Najmniej popularny ze względu na brak bindingów Pythonoych\n",
    "        - Nie wspiera automatycznego różniczkowania\n",
    "\n",
    "    - [MXNet](http://mxnet.io/) (**Amazon**)\n",
    "        - Python, C++, Go, Julia, Scala, R, ...\n",
    "        - Najwydajniejszy pod względem pamięci \n",
    "        - Od niedawna wspierany przez Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"img/tf-logo-2.png\"> \n",
    "<img style=\"float: left;\" src=\"img/th-logo.png\">\n",
    "<img style=\"float: left;\" src=\"img/torch-logo-f.png\">\n",
    "<img style=\"float: left;\" src=\"img/mx-logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wszystkie z tych narzędzi są rozwijane jako proejkty OpenSource:\n",
    "\n",
    "    - https://github.com/tensorflow/tensorflow\n",
    "\n",
    "    - https://github.com/torch/torch7\n",
    "\n",
    "    - https://github.com/Theano/Theano\n",
    "\n",
    "    - https://github.com/dmlc/mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wymienione wyżej narzędzia to zazwyczaj biblioteki operujące na dosyć niskim poziomie abstrakcji.\n",
    "\n",
    "- Wokół nich powstało wiele projektów mających jeszcze bardziej ułatwić użytkownikom uczenie sieci neuronowych.\n",
    "\n",
    "- Najpopularniejszym z takich projektów jest obecnie **Keras**, na którym skupimy się w dalszej części\n",
    "\n",
    "![k](img/keras-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.) Wstep do obliczeń symbolicznych\n",
    "\n",
    "- Sercem większości frameworków wysokiego poziomu jest jeden z wyżej wymienionych silników.\n",
    "- Keras, którego będziemy dzisiaj używać, posiada dwa takie backendy: Tensorflow oraz Theano\n",
    "- Warto poświęcić chwilę, żeby zapoznać się z najbardziej podstawowymi zastadami ich działania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Tensorflow i Theano to przykłady narzędzi, które tworzą graf operacji symbolicznych.\n",
    "\n",
    "- Oznacza to, że operacje wykonywane na zmiennych nie mają natychmiastowego efektu. Są jedynie dodawane do grafu.\n",
    "\n",
    "- Następnie graf jest kompilowany i wykonwyany."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.Variable(2.)\n",
    "b = tf.Variable(2.)\n",
    "result = a + b\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`result` nie jest równe 4! \n",
    "\n",
    "Jest tylko węzłem w grafie, symbolizującym operację dodania zmiennej `a` do `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:900px;height:300px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.4947157476537979&quot;).pbtxt = 'node {\\n  name: &quot;Variable/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;Variable/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;Variable_1/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Variable/read&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.4947157476537979&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(width=900, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbudowany graf możemy \"wykonać\", jako parametr `outptus` podając te zmienne, które chcemy obliczyć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers3 import execute_tf_graph\n",
    "\n",
    "execute_tf_graph(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobrze, ale co zrobić, jeżeli chcemy obliczyć sumę `2 + 3`. Czy musimy zbudować i skompilować cały graf od nowa?\n",
    "\n",
    "Na szczęście nie -- możemy podać wartości dowolnej zmiennej w grafie przy jego wywołaniu.\n",
    "\n",
    "Aby to zrobić, tworzymy słownik, który przypisze wybranym zmiennym odpowiednie wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "    a: 2,\n",
    "    b: 3\n",
    "}\n",
    "\n",
    "execute_tf_graph(outputs=result, inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że w słowniku tym nie podajemy nazw zmiennych (typu `str`), ale obiekty `Pythona`!\n",
    "\n",
    "**Uwaga!**: w praktyce, jeżeli chcemy, żeby nasza zmienna była inicjalizowana dopiero przy wywołaniu grafu, \n",
    "oraz żeby jej podanie było obowiązkowe, należy **tf.Variable** zamienić na **tf.placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must feed a value for placeholder tensor 'zmienna_a' with dtype float\n",
      "\t [[Node: zmienna_a = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "a = tf.placeholder(dtype=np.float32, name='zmienna_a')\n",
    "result = a + 1\n",
    "\n",
    "try:\n",
    "    execute_tf_graph(result)\n",
    "except tf.python.errors.InvalidArgumentError as e:\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### zadanie 1: \n",
    "    - oblicz kwadraty liczb 2, 3 i 4 używając tensorflow. Wykorzystaj mechanizm placeholderów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### rozwiązanie 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0]\n",
      "[9.0]\n",
      "[16.0]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(np.float32)\n",
    "result = x * x\n",
    "\n",
    "for i in {2, 3, 4}:\n",
    "    outputs = result\n",
    "    inputs = {\n",
    "        x: i\n",
    "    }\n",
    "    \n",
    "    returned = execute_tf_graph(outputs=result, inputs=inputs)\n",
    "    print(returned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### zadanie 2: \n",
    "    - dodaj dwie macierze jednostkowe 2x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### rozwiązanie 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.,  0.],\n",
       "        [ 0.,  2.]], dtype=float32)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.placeholder(dtype=np.float32)\n",
    "B = tf.placeholder(dtype=np.float32)\n",
    "result = A + B\n",
    "\n",
    "inputs = {\n",
    "    A: np.array([\n",
    "            [1, 0],\n",
    "            [0, 1]\n",
    "        ]),\n",
    "    B: np.array([\n",
    "            [1, 0],\n",
    "            [0, 1]\n",
    "        ])\n",
    "}\n",
    "\n",
    "execute_tf_graph(outputs=[result], inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Warto zaznaczyć, iż **Tensorflow został stworzony dla operacji na macierzach (tensorach)**. Radzi więc sobie wspaniale z wielowymiarowymi danymi. (Patrz rozw. powyżej)\n",
    "\n",
    "- Jakie są jednak konkretne zalety tego symbolicznego podejścia? Trzy najważniejsze to:\n",
    "    1. Optymalizacja: znając cały graf, kompilator może zoptymalizować wykonywane operacje\n",
    "    2. Współbieżność: kompilator sam zadba o to, aby wykonać obliczenia równolegle\n",
    "    3. Niezależność od architektury: Znając graf, kompilator może wygenerować kod dla CPU / GPU / FPGA etc.\n",
    "    \n",
    "- Szczególnie punkt trzeci jest tak istotny, gdyż obecnie do uczenia sieci neuronowych niemalże niezbędny jest procesor graficzny wspierający technologię `CUDA`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czym is Keras?\n",
    "\n",
    "Za https://keras.io:\n",
    "\n",
    "> ### Keras: Deep Learning library for Theano and TensorFlow\n",
    ">\n",
    "> Keras is a **high-level neural networks** library, written in **Python** and capable of running on top of either **TensorFlow** or **Theano**. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "- Jest to biblioteka mająca ułatwić tworzenie sieci neuronowych, operująca na wyższym poziomie abstrakcji niż tensorflow / theano\n",
    "- Jako backendu obliczeniowego może wykorzystywać jeden z wyżej wymienionych silników\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dlaczego Keras?\n",
    "\n",
    "- Łatwy do opanowania\n",
    "- Czytelny\n",
    "- Popularny i aktywnie rozwijany \n",
    "- Dobrze udokumentowany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak nauczyć sieć neuronową?\n",
    "\n",
    "Będziemy potrzebować trzech elementów:\n",
    "1. Dane\n",
    "2. Architektura sieci\n",
    "3. Metoda optymalizacji\n",
    "    - Funkcja straty\n",
    "    - Algorytm optymalizacyjny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Dane\n",
    "\n",
    "- Keras współpracuje z numpy, w związku z tym można korzystać z dowolnego zbioru danych, który jesteśmy w stanie wczytać do pamięci.\n",
    "- Może korzystać ze zbiorów zbyt dużych, żeby zmieśćić się w pamięci: potrzebna jest wtedy biblioteka `h5py`\n",
    "- Posiada także kilka wbudowanych zbiorów danych, jak np. mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "- klasyczny benchmark w computer vision (ponad 700 cytowań w pracach naukowych)\n",
    "- 60 000 obrazów przedstawiających cyfry napisane ludzką ręką\n",
    "- Można załadować bezpośrednio z kerasa\n",
    "\n",
    "![mnist](img/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "img_size = 28\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowanie danych: \n",
    "    - Obrazy, które trafiają do sieci neuronowej muszą mieć odpowiedni \"kształt\" (shape) oraz typ: \n",
    "        - (liczba_przykładóœ, szerokość, wysokość, liczba-kanałóœ) \n",
    "        - typem danych powinien być float32, a elementy macierzy powinny być w zakresie [0., 1.]\n",
    "    - Etykiety (informacja, którą cyfrę przedstawia dany obrazek), powinny mieć kształt (liczba_przykładów, liczba_cyfr) patrz poniżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras oczekuje, iż etykiety będą macierzą binarną o kształcie (liczba_przykładów, liczba_możliwych_etykiet)\n",
    "# Mówimy, że dany obraz należy do klasy **i**, kiedy w kolumnie **i-tej** znajduje się 1\n",
    "# przykład:\n",
    "etykiety = [\n",
    "    0,\n",
    "    1,\n",
    "    2\n",
    "]\n",
    "\n",
    "etykiety_keras = [\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# aby zamienić etykiety wczytane przez mnist.load_data(), skorzystamy z funkcji pomocniczej kerasa \"to_categorical\":\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Przygotujmy też same obrazki\n",
    "rozmiar = 20\n",
    "X_train = X_train.reshape(-1, rozmiar, rozmiar, 1)\n",
    "X_test  = X_test.reshape(-1,  rozmiar, rozmiar, 1)\n",
    "\n",
    "X_train = X_train.astype(np.float32) / 255.\n",
    "X_test  = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sprawdzźmy, jak wyglądają nasze dane:\n",
    "i = np.random.randint(0, y_train.shape[0])\n",
    "\n",
    "print(\"Label: \", y_train[i])\n",
    "sns.heatmap(X_train[i].reshape(rozmiar, rozmiar));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Architektura sieci\n",
    "- Keras wspiera dwie metody definiowania architektury: model liniowy i API funkcjonalne.\n",
    "- Skupimy się dzisiaj głównie na modelach liniowych, ponieważ w zupełności wystarczają one do większośći zastosowań praktycznych.\n",
    "- Kluczowym pojęciem przy definiowaniu architektury jest pojęcie **warstwy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Warstwy:\n",
    "\n",
    "- Podstawowym elementem, z którego budowana jest sieć neuronowa, jest warstwa. \n",
    "- Model liniowy w kerasie to nic innego, jak tylko złożenie kolejnych, następujących po sobie warstw.\n",
    "- Warstwa (layer) jest podstawową jednostką przetwarzania: \n",
    "- przyjmuje ona jakąś macierz (tensor) na wejściu, modyfikuje ją, a następnie podaje na wyściu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aby zobrazować to w praktyce, zaimplementujemy regresję logistyczną, poznaną na poprzednich zajęciach, z użyciem kerasa\n",
    "\n",
    "# zdefiniujmy sobie najpierw warstwy, których użyjemy:\n",
    "\n",
    "# regresja logistyczna oczekuje jednowymiarowego wektora cech. Obrazki to macierze kwadratowe, należy je \"słaszczyć\"\n",
    "flatten_layer = Flatten(input_shape=(rozmiar, rozmiar, 1))\n",
    "\n",
    "# następnie wektor cech jest mnożony przez macierz wag. Macierz wag ma rozmiar (liczba_cech x liczba_klas)\n",
    "# Keras sam odgadnie, jaka jest liczba cech! Podajemy jedynie liczbę klas.\n",
    "# Warstwą realizującą mnożenie macierzy jest warstwa Dense:\n",
    "matmul_layer = Dense(10)\n",
    "\n",
    "# Teraz nasz model pobiera na wejściu obrazek 20x20, a generuje wektor o rozmiarze (10, ), \n",
    "# gdzie każdy element tego wektora to \"wynik\" danej klasy. Należy zamienić jeszcze ten wynik na prawdopodobieństwa\n",
    "get_probas_layer = Activation(\"softmax\")\n",
    "\n",
    "# I to wszystko! Zbudujmy model ze zdefiniowanych wyżej warstw\n",
    "model = Sequential()\n",
    "model.add(flatten_layer)\n",
    "model.add(matmul_layer)\n",
    "model.add(get_probas_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metoda optymalizacji\n",
    "- Keras musi wiedzieć, jaką funkcję straty chcemy minimalizować.\n",
    "- Musi także wiedzieć, jakiej metody ma użyć do minimalizacji\n",
    "- Dobra wiadomość: nie musimy umieć implementować tych elementów: keras o wszystko zadba!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skorzystamy ze standardowego zestawu w przypadku klasyfikacji: straty logistycznej i optymalizatora [Adam](google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_straty = \"categorical_crossbentropy\"\n",
    "optymalizator = \"adam\"\n",
    "\n",
    "# Model należy jeszcze skompilować, podając wybraną stratę i optymalizator\n",
    "model.compile(loss=f_straty, optimizer=optymalizator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uczenie:\n",
    "- Nasz model jest już kompletny!\n",
    "- Teraz wystarczy wywołać funkcję, która rozpocznie proces uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liczba_iteracji = 5\n",
    "model.fit(X_train, Y_train, nb_epoch=liczba_iteracji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jak sobie poradziliśmy?\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# TODO\n",
    "plot_confmat(preds, Y_test)\n",
    "plot_accuracy(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gdzie są sieci neuronowe?!\n",
    "\n",
    "- Powyższy model to zwykła regresja logistyczna w kerasie. Jak przejść od niej do uczenia głębokiego? \n",
    "- Proste: dodać więcej warstw!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(rozmiar, rozmiar, 1)))\n",
    "\n",
    "# pomiędzy warstwami mnożącymi macierze powinny znajdować się funkcje aktywacji\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\")) \n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\")) \n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"softmax\")) \n",
    "\n",
    "model.compile('adam', 'crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liczba_iteracji = 5\n",
    "model.fit(X_train, Y_train, nb_epoch=liczba_iteracji)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# TODO\n",
    "plot_confmat(preds, Y_test)\n",
    "plot_accuracy(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sieci konwolucyjne\n",
    "- Dotychczas jako podstawowych elementów przetwarzania używaliśmy warstw, które wykonują proste mnożenie macierzy\n",
    "- Istnieją warstwy, które dedykowane są konkretnie rozpoznawaniu obrazów: tzw. warstwy konwolucyjne.\n",
    "- Dobieranie parametrów tych warstw to bardziej sztuka niż nauka i wymaga sporej wprawy. Dlatego nie zjamiemy się tym na tych zajęciach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Przykład sieci z warstwami konwolucyjnymi\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode='valid', input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liczba_iteracji = 5\n",
    "model.fit(X_train, Y_train, nb_epoch=liczba_iteracji)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# TODO\n",
    "plot_confmat(preds, Y_test)\n",
    "plot_accuracy(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Prawdziwa\" sieć neuronowa\n",
    "- Keras udostępnia gotowe, wyuczone już modele.\n",
    "- Są to potężne sieci neuronowe, uczone tygodniami na olbrzymim zbiorze danych [Imagenet](google.com)\n",
    "- Możemy wykorzystać tę sieć i szybko dostosować ją do własnych potrzeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = helpers3.load_psy_koty()\n",
    "model = helpers3.load_vgg_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Teraz, kiedy znamy już podstawowe zasady rządzące Kerasem\n",
    "# Pokażemy, w jak łatwy sposób można wykorzystać to narzędzie to rozpoznawania obrazu.\n",
    "\n",
    "# W sposób trywialnie prosty otrzymamy wynik, który jeszcze parę lat temu pozostawał poza zasięgiem najbardziej zaawansowanych metod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
